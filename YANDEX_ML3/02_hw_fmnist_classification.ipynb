{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDsVMGiVgSq2"
   },
   "source": [
    "## Классификация FashionMNIST\n",
    "\n",
    "##### Автор: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), https://t.me/s/girafe_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "3isBRG6PgSq6"
   },
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from tqdm.notebook import tqdm\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "def get_predictions(model, eval_data, step=10):\n",
    "\n",
    "    predicted_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, len(eval_data), step):\n",
    "            y_predicted = model(eval_data[idx : idx + step].to(device))\n",
    "            predicted_labels.append(y_predicted.argmax(dim=1).cpu())\n",
    "\n",
    "    predicted_labels = torch.cat(predicted_labels)\n",
    "    predicted_labels = \",\".join([str(x.item()) for x in list(predicted_labels)])\n",
    "    return predicted_labels\n",
    "\n",
    "\n",
    "def get_accuracy(model, data_loader):\n",
    "    predicted_labels = []\n",
    "    real_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            y_predicted = model(batch[0].to(device))\n",
    "            predicted_labels.append(y_predicted.argmax(dim=1).cpu())\n",
    "            real_labels.append(batch[1])\n",
    "\n",
    "    predicted_labels = torch.cat(predicted_labels)\n",
    "    real_labels = torch.cat(real_labels)\n",
    "    accuracy_score = (predicted_labels == real_labels).type(torch.FloatTensor).mean()\n",
    "    return accuracy_score\n",
    "\n",
    "\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите файл `hw_overfitting_data_dict.npy` (ссылка есть на странице с заданием), он понадобится для генерации посылок. Код ниже может его загрузить (но в случае возникновения ошибки скачайте и загрузите его вручную).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/girafe-ai/ml-course/raw/24f_ysda/homeworks/hw_overfitting/hw_overfitting_data_dict -O hw_overfitting_data_dict.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "assert os.path.exists(\n",
    "    \"hw_overfitting_data_dict.npy\"\n",
    "), \"Please, download `hw_overfitting_data_dict.npy` and place it in the working directory\"\n",
    "\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeA6Q5-CgSq7"
   },
   "source": [
    "Вернемся к задаче распознавания простых изображений, рассмотренной ранее. Но теперь будем работать с набором данных [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist). В данном задании воспользуемся всем датасетом целиком.\n",
    "\n",
    "__Ваша первая задача: реализовать весь пайплан обучения модели и добиться качества $\\geq 88.5\\%$ на тестовой выборке.__\n",
    "\n",
    "Код для обучения модели в данном задании отсутствует. Присутствует лишь несколько тестов, которые помогут вам отладить свое решение. За примером можно обратиться к ноутбукам с предыдущих занятий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_DEVICE_ID = 0  # change if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nPG1KbQAgl8b"
   },
   "outputs": [],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "device = (\n",
    "    torch.device(f\"cuda:{CUDA_DEVICE_ID}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomResizedCrop(28, scale=(0.9, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    # Нормализуем данные – это может помочь оптимизации\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 809
    },
    "id": "aYcL28OsgSq8",
    "outputId": "93aafa07-fb56-43bd-f928-918f45fe30e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Image label: 6')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "\n",
    "train_fmnist_data = FashionMNIST(\n",
    "    \".\", train=True, transform=torchvision.transforms.ToTensor(), download=True\n",
    ")\n",
    "test_fmnist_data = FashionMNIST(\n",
    "    \".\", train=False, transform=torchvision.transforms.ToTensor(), download=True\n",
    ")\n",
    "\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_fmnist_data, batch_size=32, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_fmnist_data, batch_size=32, shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "random_batch = next(iter(train_data_loader))\n",
    "_image, _label = random_batch[0][0], random_batch[1][0]\n",
    "plt.figure()\n",
    "plt.imshow(_image.reshape(28, 28))\n",
    "plt.title(f\"Image label: {_label}\")\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6jWRv1rgSq8"
   },
   "source": [
    "Постройте модель ниже. Пожалуйста, не стройте переусложненную сеть, не стоит делать ее глубже четырех слоев (можно и меньше). Ваша основная задача – обучить модель и получить качество на отложенной (тестовой выборке) не менее 88.5% accuracy.\n",
    "\n",
    "__Внимание, ваша модель должна быть представлена именно переменной `model_task_1`. На вход ей должен приходить тензор размерностью (1, 28, 28).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "'''class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Первый сверточный блок: увеличиваем число каналов до 64\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(64)\n",
    "        self.pool  = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Второй сверточный блок: 64 -> 128 каналов\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Третий сверточный блок: 128 -> 256 каналов\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3   = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Слои Dropout для регуляризации\n",
    "        self.dropout_conv = nn.Dropout(0.3)\n",
    "        \n",
    "        # Вычисляем размер выхода после сверточных блоков.\n",
    "        # После двух пуллингов (каждый сокращает размер в 2 раза): 28x28 → 14x14 → 7x7.\n",
    "        self.fc1   = nn.Linear(256 * 7 * 7, 256)\n",
    "        self.dropout_fc = nn.Dropout(0.5)\n",
    "        self.fc2   = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x имеет форму (batch_size, 1, 28, 28)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout_conv(x)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout_conv(x)\n",
    "        \n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        # Без дополнительного пула – размер уже 7x7\n",
    "        x = x.view(x.size(0), -1)  # преобразуем в вектор\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout_fc(x)\n",
    "        logits = self.fc2(x)\n",
    "        return logits'''\n",
    "    \n",
    "\n",
    "\n",
    "'''class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 256)  # первый полносвязный слой\n",
    "        self.fc2 = nn.Linear(256, 128)      # второй полносвязный слой\n",
    "        self.fc3 = nn.Linear(128, 10)       # выходной слой\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x имеет форму (batch_size, 1, 28, 28)\n",
    "        x = x.view(x.size(0), -1)  # преобразуем в (batch_size, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)  # возвращаются логиты для 10 классов\n",
    "        return logits'''\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Первый сверточный блок: Conv -> BatchNorm -> ReLU -> MaxPool\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Второй сверточный блок: Conv -> BatchNorm -> ReLU -> MaxPool\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Полносвязные слои для классификации\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # после двух пулов размер 28x28 -> 7x7\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x имеет форму (batch_size, 1, 28, 28)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # разворачиваем в вектор\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc2(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Creating model instance\n",
    "model_task_1 = Discriminator()\n",
    "print(model_task_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAoLV4dkoy5M"
   },
   "source": [
    "Не забудьте перенести модель на выбранный `device`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Xas9SIXDoxvZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_task_1.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pLRWysggSq9"
   },
   "source": [
    "Локальные тесты для проверки вашей модели доступны ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_qMQzo1ggSq9",
    "outputId": "c00008eb-ef88-4000-ce47-e8dedd26e061"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything seems fine!\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "assert model_task_1 is not None, \"Please, use `model_task_1` variable to store your model\"\n",
    "\n",
    "try:\n",
    "    x = random_batch[0].to(device)\n",
    "    y = random_batch[1].to(device)\n",
    "\n",
    "    # compute outputs given inputs, both are variables\n",
    "    y_predicted = model_task_1(x)\n",
    "except Exception as e:\n",
    "    print(\"Something is wrong with the model\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "assert y_predicted.shape[-1] == 10, \"Model should predict 10 logits/probas\"\n",
    "\n",
    "print(\"Everything seems fine!\")\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suRmIPwIgSq9"
   },
   "source": [
    "Настройте параметры модели на обучающей выборке. Также рекомендуем поработать с `learning rate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8426\n",
      "0.8426\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "data = []\n",
    "y = []\n",
    "\n",
    "for image, label in train_fmnist_data:\n",
    "    data.append(np.ravel(image))\n",
    "    y.append(label)\n",
    "\n",
    "    \n",
    "data = np.array(data)\n",
    "y = np.array(y)\n",
    "\n",
    "ss = StandardScaler().fit(data)\n",
    "pca = PCA(n_components=200).fit(data)\n",
    "data = ss.transform(data)\n",
    "data_pca = pca.transform(data)\n",
    "\n",
    "data_pca\n",
    "test_data = []\n",
    "test_y = []\n",
    "\n",
    "for image, label in test_fmnist_data:\n",
    "    test_data.append(np.ravel(image))\n",
    "    test_y.append(label)\n",
    "    \n",
    "test_data = np.array(test_data)\n",
    "test_y = np.array(test_y)\n",
    "test_data = ss.transform(test_data)\n",
    "test_data_pca = pca.transform(test_data)\n",
    "\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(data_pca,y)\n",
    "\n",
    "print(model.score(test_data_pca,test_y.reshape(-1,1)))\n",
    "print(accuracy_score(test_y.reshape(-1,1),model.predict(test_data_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc812e2ba02467598274345136a8460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.4111\n",
      "Epoch [2/20], Loss: 0.2849\n",
      "Epoch [3/20], Loss: 0.2442\n",
      "Epoch [4/20], Loss: 0.2192\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Прямой проход\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model_task_1(images)\n\u001b[0;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Обратное распространение ошибки и оптимизация\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[35], line 81\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m# x имеет форму (batch_size, 1, 28, 28)\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[1;32m---> 81\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)\n\u001b[0;32m     82\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[0;32m     83\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\pooling.py:213\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mmax_pool2d(\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding,\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation,\n\u001b[0;32m    219\u001b[0m         ceil_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mceil_mode,\n\u001b[0;32m    220\u001b[0m         return_indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_indices,\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_jit_internal.py:624\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\functional.py:830\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 830\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28minput\u001b[39m, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_task_1.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20  # можно увеличить число эпох для повышения качества\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model_task_1.train()  # переводим модель в режим обучения\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_data_loader:\n",
    "        # Переносим входные данные на устройство\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Обнуляем градиенты\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Прямой проход\n",
    "        outputs = model_task_1(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Обратное распространение ошибки и оптимизация\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_data_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'optimizer = torch.optim.AdamW(model_task_1.parameters(), lr=0.001, weight_decay=1e-4)\\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\\ncriterion = nn.CrossEntropyLoss()\\n\\nnum_epochs = 20  # увеличиваем число эпох для лучшего обучения\\n\\nfor epoch in tqdm(range(num_epochs),\\'Epoch\\'):\\n    model_task_1.train()\\n    running_loss = 0.0\\n    for images, labels in train_data_loader:\\n        images, labels = images.to(device), labels.to(device)\\n        optimizer.zero_grad()\\n        outputs = model_task_1(images)\\n        loss = criterion(outputs, labels)\\n        loss.backward()\\n        optimizer.step()\\n        running_loss += loss.item()\\n        \\n    scheduler.step()  # корректируем learning rate\\n    avg_loss = running_loss / len(train_data_loader)\\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''optimizer = torch.optim.AdamW(model_task_1.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 20  # увеличиваем число эпох для лучшего обучения\n",
    "\n",
    "for epoch in tqdm(range(num_epochs),'Epoch'):\n",
    "    model_task_1.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_task_1(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    scheduler.step()  # корректируем learning rate\n",
    "    avg_loss = running_loss / len(train_data_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zce7gt1gSq-"
   },
   "source": [
    "Также, напоминаем, что в любой момент можно обратиться к замечательной [документации](https://pytorch.org/docs/stable/index.html) и [обучающим примерам](https://pytorch.org/tutorials/).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usswrWYOgSq-"
   },
   "source": [
    "Оценим качество классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xua3TVZHgSq-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy on train set: 0.94202\n"
     ]
    }
   ],
   "source": [
    "train_acc_task_1 = get_accuracy(model_task_1, train_data_loader)\n",
    "print(f\"Neural network accuracy on train set: {train_acc_task_1:3.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9KEKXBxgSq-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy on test set: 0.8889\n"
     ]
    }
   ],
   "source": [
    "test_acc_task_1 = get_accuracy(model_task_1, test_data_loader)\n",
    "print(f\"Neural network accuracy on test set: {test_acc_task_1:3.5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oyhmMobgSq_"
   },
   "source": [
    "Проверка, что необходимые пороги пройдены:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "id": "OAIrURCEgSq_",
    "outputId": "7c983690-a92e-4693-89fb-7c86c002921a"
   },
   "outputs": [],
   "source": [
    "assert test_acc_task_1 >= 0.885, \"Train accuracy is below 0.885 threshold\"\n",
    "assert (\n",
    "    train_acc_task_1 >= 0.905\n",
    "), \"Train accuracy is below 0.905 while test accuracy is fine. We recommend to check your model and data flow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обращаем внимане, код ниже предполагает, что ваша модель имеет содержится в переменной `model_task_1`, а файл `hw_fmnist_data_dict.npy` находится в той же директории, что и ноутбук (он доступен в репозитории)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to `submission_dict_fmnist_task_1.json`\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "assert os.path.exists(\n",
    "    \"hw_fmnist_data_dict.npy\"\n",
    "), \"Please, download `hw_fmnist_data_dict.npy` and place it in the working directory\"\n",
    "\n",
    "loaded_data_dict = np.load(\"hw_fmnist_data_dict.npy\", allow_pickle=True)\n",
    "\n",
    "submission_dict = {\n",
    "    \"train_predictions_task_1\": get_predictions(\n",
    "        model_task_1, torch.FloatTensor(loaded_data_dict.item()[\"train\"])\n",
    "    ),\n",
    "    \"test_predictions_task_1\": get_predictions(\n",
    "        model_task_1, torch.FloatTensor(loaded_data_dict.item()[\"test\"])\n",
    "    ),\n",
    "}\n",
    "\n",
    "with open(\"submission_dict_fmnist_task_1.json\", \"w\") as iofile:\n",
    "    json.dump(submission_dict, iofile)\n",
    "print(\"File saved to `submission_dict_fmnist_task_1.json`\")\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сдача задания\n",
    "Сдайте сгенерированный файл в соответствующую задачу в соревновании, а именно:\n",
    "    \n",
    "* `submission_dict_fmnist_task_1.json` в задачу Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtWnYAN_gSrA"
   },
   "source": [
    "На этом задание завершено. Поздравляем!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
