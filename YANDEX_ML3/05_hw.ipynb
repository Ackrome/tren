{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a015fbda",
   "metadata": {},
   "source": [
    "# HW_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f331f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "\n",
      "Training data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1288 entries, 0 to 1287\n",
      "Data columns (total 17 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   area           1288 non-null   int64  \n",
      " 1   perimeter      1288 non-null   float64\n",
      " 2   major_axis     1288 non-null   float64\n",
      " 3   minor_axis     1288 non-null   float64\n",
      " 4   eccentricity   1288 non-null   float64\n",
      " 5   eqdiasq        1288 non-null   float64\n",
      " 6   solidity       1288 non-null   float64\n",
      " 7   convex_area    1288 non-null   int64  \n",
      " 8   extent         1288 non-null   float64\n",
      " 9   aspect_ratio   1288 non-null   float64\n",
      " 10  roundness      1288 non-null   float64\n",
      " 11  compactness    1288 non-null   float64\n",
      " 12  shapefactor_1  1288 non-null   float64\n",
      " 13  shapefactor_2  1288 non-null   float64\n",
      " 14  shapefactor_3  1288 non-null   float64\n",
      " 15  shapefactor_4  1288 non-null   float64\n",
      " 16  target         1288 non-null   int64  \n",
      "dtypes: float64(14), int64(3)\n",
      "memory usage: 171.2 KB\n",
      "\n",
      "First 5 rows of training data:\n",
      "    area  perimeter  major_axis  minor_axis  eccentricity   eqdiasq  solidity  \\\n",
      "0  75516  1731.4840    411.7352    245.7620        0.8023  310.0806    0.9148   \n",
      "1  98903  1374.4370    477.2451    269.7676        0.8249  354.8622    0.9585   \n",
      "2  84746  1311.1570    482.7735    235.9040        0.8725  328.4843    0.9121   \n",
      "3  98184  1463.1680    434.3769    292.6472        0.7390  353.5700    0.9543   \n",
      "4  94170  1267.7271    440.1109    278.4162        0.7745  346.2672    0.9643   \n",
      "\n",
      "   convex_area  extent  aspect_ratio  roundness  compactness  shapefactor_1  \\\n",
      "0        82546  0.7169        1.6753     0.3165       0.7531         0.0055   \n",
      "1       103181  0.7679        1.7691     0.6579       0.7436         0.0048   \n",
      "2        92914  0.7162        2.0465     0.6195       0.6804         0.0057   \n",
      "3       102890  0.7316        1.4843     0.5763       0.8140         0.0044   \n",
      "4        97656  0.6836        1.5808     0.7363       0.7868         0.0047   \n",
      "\n",
      "   shapefactor_2  shapefactor_3  shapefactor_4  target  \n",
      "0         0.0033         0.5672         0.9502       1  \n",
      "1         0.0027         0.5529         0.9781       0  \n",
      "2         0.0028         0.4630         0.9474       1  \n",
      "3         0.0030         0.6625         0.9834       0  \n",
      "4         0.0030         0.6190         0.9785       0  \n",
      "\n",
      "Test data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 430 entries, 0 to 429\n",
      "Data columns (total 16 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   area           430 non-null    int64  \n",
      " 1   perimeter      430 non-null    float64\n",
      " 2   major_axis     430 non-null    float64\n",
      " 3   minor_axis     430 non-null    float64\n",
      " 4   eccentricity   430 non-null    float64\n",
      " 5   eqdiasq        430 non-null    float64\n",
      " 6   solidity       430 non-null    float64\n",
      " 7   convex_area    430 non-null    int64  \n",
      " 8   extent         430 non-null    float64\n",
      " 9   aspect_ratio   430 non-null    float64\n",
      " 10  roundness      430 non-null    float64\n",
      " 11  compactness    430 non-null    float64\n",
      " 12  shapefactor_1  430 non-null    float64\n",
      " 13  shapefactor_2  430 non-null    float64\n",
      " 14  shapefactor_3  430 non-null    float64\n",
      " 15  shapefactor_4  430 non-null    float64\n",
      "dtypes: float64(14), int64(2)\n",
      "memory usage: 53.9 KB\n",
      "\n",
      "First 5 rows of test data:\n",
      "    area  perimeter  major_axis  minor_axis  eccentricity   eqdiasq  solidity  \\\n",
      "0  93313  1862.7260    447.1666    278.4893        0.7824  344.6880    0.9361   \n",
      "1  78778  2159.5969    439.1004    240.0113        0.8374  316.7069    0.9340   \n",
      "2  74757  1661.6720    441.6910    225.3914        0.8600  308.5183    0.9359   \n",
      "3  88074  2199.8889    460.8836    283.3717        0.7886  334.8721    0.8709   \n",
      "4  79318  2589.4900    429.3032    279.5817        0.7589  317.7905    0.8666   \n",
      "\n",
      "   convex_area  extent  aspect_ratio  roundness  compactness  shapefactor_1  \\\n",
      "0        99678  0.7607        1.6057     0.3380       0.7708         0.0048   \n",
      "1        84345  0.7265        1.8295     0.2123       0.7213         0.0056   \n",
      "2        79873  0.7255        1.9597     0.3402       0.6985         0.0059   \n",
      "3       101133  0.6922        1.6264     0.2287       0.7266         0.0052   \n",
      "4        91528  0.6634        1.5355     0.1486       0.7402         0.0054   \n",
      "\n",
      "   shapefactor_2  shapefactor_3  shapefactor_4  \n",
      "0         0.0030         0.5942         0.9541  \n",
      "1         0.0030         0.5202         0.9517  \n",
      "2         0.0030         0.4879         0.9561  \n",
      "3         0.0032         0.5279         0.8586  \n",
      "4         0.0035         0.5480         0.8414  \n",
      "\n",
      "Missing values in training data:\n",
      "0\n",
      "\n",
      "Missing values in test data:\n",
      "0\n",
      "\n",
      "Train and test columns match.\n",
      "\n",
      "Data scaled. Training features shape: (1288, 16), Test features shape: (430, 16)\n",
      "\n",
      "Performing cross-validation...\n",
      "Cross-validation accuracy scores: [0.83333333 0.87209302 0.87984496 0.85992218 0.86381323]\n",
      "Mean cross-validation accuracy: 0.8618\n",
      "\n",
      "Training the final model on the full training data...\n",
      "Model training complete.\n",
      "\n",
      "Making predictions on the test data...\n",
      "Predictions made.\n",
      "Example predictions: [0 1 1 0 0 1 1 1 0 1]\n",
      "\n",
      "Predictions saved to answers.csv\n",
      "\n",
      "Estimated model accuracy (from CV): 0.8618\n",
      "CV Accuracy is below the minimum threshold (86.5%). Consider model tuning or different models.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load the datasets\n",
    "try:\n",
    "    train_df = pd.read_csv('train.csv')\n",
    "    test_df = pd.read_csv('test.csv')\n",
    "    print(\"Data loaded successfully.\")\n",
    "    # Display basic info about the dataframes\n",
    "    print(\"\\nTraining data info:\")\n",
    "    train_df.info()\n",
    "    print(\"\\nFirst 5 rows of training data:\")\n",
    "    print(train_df.head())\n",
    "    \n",
    "    print(\"\\nTest data info:\")\n",
    "    test_df.info()\n",
    "    print(\"\\nFirst 5 rows of test data:\")\n",
    "    print(test_df.head())\n",
    "\n",
    "    # Check for missing values\n",
    "    print(\"\\nMissing values in training data:\")\n",
    "    print(train_df.isnull().sum().sum()) # Check total missing values\n",
    "    print(\"\\nMissing values in test data:\")\n",
    "    print(test_df.isnull().sum().sum()) # Check total missing values\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: train.csv or test.csv not found. Make sure the files are in the correct directory.\")\n",
    "    # Create dummy dataframes for the rest of the script to run without errors\n",
    "    # In a real scenario, you would stop execution here or handle the error appropriately.\n",
    "    train_df = pd.DataFrame(np.random.rand(100, 10), columns=[f'feature_{i}' for i in range(9)] + ['target'])\n",
    "    train_df['target'] = np.random.randint(0, 2, 100)\n",
    "    test_df = pd.DataFrame(np.random.rand(50, 9), columns=[f'feature_{i}' for i in range(9)])\n",
    "    print(\"\\n--- Created dummy dataframes for demonstration ---\")\n",
    "    \n",
    "# Prepare the data\n",
    "# Assuming the last column in train.csv is the target and the rest are features.\n",
    "# Assuming test.csv has the same feature columns as train.csv (excluding the target).\n",
    "\n",
    "if 'target' in train_df.columns:\n",
    "    X_train_full = train_df.drop('target', axis=1)\n",
    "    y_train_full = train_df['target']\n",
    "    X_test = test_df \n",
    "    \n",
    "    # Verify columns match (excluding target)\n",
    "    if list(X_train_full.columns) == list(X_test.columns):\n",
    "        print(\"\\nTrain and test columns match.\")\n",
    "        \n",
    "        # --- Preprocessing ---\n",
    "        # Scale numerical features. Assuming all features are numerical.\n",
    "        # If there were categorical features, they would need encoding first.\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_full)\n",
    "        X_test_scaled = scaler.transform(X_test) # Use the scaler fitted on training data\n",
    "        \n",
    "        print(f\"\\nData scaled. Training features shape: {X_train_scaled.shape}, Test features shape: {X_test_scaled.shape}\")\n",
    "\n",
    "        # --- Model Selection and Training ---\n",
    "        # Let's use a robust model like RandomForestClassifier\n",
    "        model = RandomForestClassifier(n_estimators=150, # Increased estimators for potentially better performance\n",
    "                                       random_state=42, \n",
    "                                       n_jobs=-1,      # Use all available CPU cores\n",
    "                                       max_depth=20,   # Limit depth to prevent overfitting\n",
    "                                       min_samples_leaf=3) # Require more samples per leaf\n",
    "\n",
    "        # --- Cross-validation (Optional but recommended) ---\n",
    "        # Evaluate model performance on the training set using cross-validation\n",
    "        # to get a better estimate of its generalization ability.\n",
    "        print(\"\\nPerforming cross-validation...\")\n",
    "        cv_scores = cross_val_score(model, X_train_scaled, y_train_full, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        print(f\"Cross-validation accuracy scores: {cv_scores}\")\n",
    "        mean_cv_accuracy = np.mean(cv_scores)\n",
    "        print(f\"Mean cross-validation accuracy: {mean_cv_accuracy:.4f}\")\n",
    "\n",
    "        # --- Final Model Training ---\n",
    "        # Train the model on the entire training dataset\n",
    "        print(\"\\nTraining the final model on the full training data...\")\n",
    "        model.fit(X_train_scaled, y_train_full)\n",
    "        print(\"Model training complete.\")\n",
    "\n",
    "        # --- Prediction ---\n",
    "        # Make predictions on the scaled test data\n",
    "        print(\"\\nMaking predictions on the test data...\")\n",
    "        predictions = model.predict(X_test_scaled)\n",
    "        print(\"Predictions made.\")\n",
    "        print(f\"Example predictions: {predictions[:10]}\") # Show first 10 predictions\n",
    "\n",
    "        # --- Output Generation ---\n",
    "        # Create the answers.csv file as required\n",
    "        output_df = pd.DataFrame(predictions, columns=None) # No header, single column\n",
    "        \n",
    "        output_filename = 'answers.csv'\n",
    "        try:\n",
    "            output_df.to_csv(output_filename, index=False, header=False)\n",
    "            print(f\"\\nPredictions saved to {output_filename}\")\n",
    "            \n",
    "            # Verification message based on CV score and target accuracy range\n",
    "            print(f\"\\nEstimated model accuracy (from CV): {mean_cv_accuracy:.4f}\")\n",
    "            if mean_cv_accuracy < 0.865:\n",
    "                 print(\"CV Accuracy is below the minimum threshold (86.5%). Consider model tuning or different models.\")\n",
    "            elif mean_cv_accuracy >= 0.865 and mean_cv_accuracy <= 0.915:\n",
    "                 print(\"CV Accuracy is within the target range (86.5% - 91.5%). Good!\")\n",
    "            else: # mean_cv_accuracy > 0.915\n",
    "                 # The prompt phrasing \"целевая точность модели (accuracy) должна быть ниже 91.5%\" is unusual.\n",
    "                 # Usually, higher accuracy is better. Assuming it might mean \"full score requires > 91.5%\"\n",
    "                 # OR it might be a very specific constraint. \n",
    "                 # Let's provide feedback based on the literal (unusual) interpretation first,\n",
    "                 # and the likely intended interpretation.\n",
    "                 print(f\"CV Accuracy ({mean_cv_accuracy:.4f}) exceeds the specified upper limit of 91.5%.\")\n",
    "                 print(\"Based on the literal requirement ('below 91.5%'), this might be too high.\")\n",
    "                 print(\"However, it's more likely the goal is *at least* 91.5% for full score. Check task clarification if possible.\")\n",
    "                 print(\"If the goal truly is <91.5%, you might need to simplify the model (e.g., reduce n_estimators/max_depth) or use more regularization.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError saving predictions to {output_filename}: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nError: Training and test feature columns do not match.\")\n",
    "        print(f\"Training columns: {list(X_train_full.columns)}\")\n",
    "        print(f\"Test columns: {list(X_test.columns)}\")\n",
    "else:\n",
    "    print(\"\\nError: 'target' column not found in train.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2086c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting GridSearchCV for RandomForestClassifier...\n",
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "Best parameters found: {'max_depth': 20, 'max_features': None, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 250}\n",
      "Best cross-validation accuracy: 0.8696\n",
      "\n",
      "Making predictions with the tuned model...\n",
      "\n",
      "Tuned predictions saved to answers_tuned.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150, 200, 250],\n",
    "    'max_depth': [10, 20, 30, None], # None means nodes expand until pure\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 3, 5],\n",
    "    'max_features': ['sqrt', 'log2', None] # None means max_features=n_features\n",
    "}\n",
    "\n",
    "# Initialize the base model\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "# cv=5 means 5-fold cross-validation\n",
    "# scoring='accuracy' specifies the metric to optimize\n",
    "# n_jobs=-1 uses all available cores\n",
    "# verbose=2 shows progress\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, \n",
    "                            scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit GridSearchCV - this will take time!\n",
    "print(\"\\nStarting GridSearchCV for RandomForestClassifier...\")\n",
    "grid_search.fit(X_train_scaled, y_train_full)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Use the best estimator found by GridSearchCV for final training and prediction\n",
    "best_model = grid_search.best_estimator_ \n",
    "\n",
    "# --- Retrain on full data (optional but good practice) ---\n",
    "# best_model.fit(X_train_scaled, y_train_full) \n",
    "# Note: GridSearchCV with refit=True (default) already retrains the best model on the whole dataset provided to .fit()\n",
    "\n",
    "# --- Prediction ---\n",
    "print(\"\\nMaking predictions with the tuned model...\")\n",
    "predictions = best_model.predict(X_test_scaled) \n",
    "\n",
    "# --- Output Generation ---\n",
    "output_df = pd.DataFrame(predictions, columns=None)\n",
    "output_filename = 'answers_tuned.csv' # Save with a new name\n",
    "output_df.to_csv(output_filename, index=False, header=False)\n",
    "print(f\"\\nTuned predictions saved to {output_filename}\")\n",
    "# You would then rename this to answers.csv for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7b8541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Train and test columns match.\n",
      "Data scaled. Training features shape: (1288, 16), Test features shape: (430, 16)\n",
      "\n",
      "Starting GridSearchCV for XGBoostClassifier...\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "Best parameters found for XGBoost: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Best cross-validation accuracy (XGBoost): 0.8734\n",
      "\n",
      "Making predictions with the tuned XGBoost model...\n",
      "\n",
      "XGBoost predictions saved to answers_xgboost.csv\n",
      "\n",
      "Estimated model accuracy (XGBoost CV): 0.8734\n",
      "XGBoost CV Accuracy is within the target range (86.5% - 91.5%). Good!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb # Import XGBoost\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore') # Suppress warnings for cleaner output\n",
    "\n",
    "# --- Load Data (Assuming files are still accessible) ---\n",
    "# It's good practice to reload or ensure data is in the expected state\n",
    "try:\n",
    "    train_df = pd.read_csv('train.csv')\n",
    "    test_df = pd.read_csv('test.csv')\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: train.csv or test.csv not found.\")\n",
    "    # Handle error or exit if necessary\n",
    "    exit() # Exit if data isn't available\n",
    "\n",
    "# --- Prepare Data ---\n",
    "if 'target' in train_df.columns and all(col in train_df.columns for col in test_df.columns):\n",
    "    X_train_full = train_df.drop('target', axis=1)\n",
    "    y_train_full = train_df['target']\n",
    "    X_test = test_df\n",
    "\n",
    "    # Ensure columns match (excluding target)\n",
    "    if list(X_train_full.columns) == list(X_test.columns):\n",
    "        print(\"Train and test columns match.\")\n",
    "        \n",
    "        # --- Preprocessing (Scaling) ---\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_full)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        print(f\"Data scaled. Training features shape: {X_train_scaled.shape}, Test features shape: {X_test_scaled.shape}\")\n",
    "\n",
    "        # --- XGBoost Model Tuning ---\n",
    "        \n",
    "        # Define the parameter grid for XGBoost\n",
    "        # Note: This grid is smaller than the RF one to reduce runtime, \n",
    "        # you might expand it for more thorough search.\n",
    "        param_grid_xgb = {\n",
    "            'n_estimators': [100, 200, 300],       # Number of boosting rounds\n",
    "            'learning_rate': [0.01, 0.1, 0.2],   # Step size shrinkage\n",
    "            'max_depth': [3, 5, 7],              # Maximum depth of a tree\n",
    "            'subsample': [0.7, 0.8, 1.0],        # Fraction of samples used per tree\n",
    "            'colsample_bytree': [0.7, 0.8, 1.0]  # Fraction of features used per tree\n",
    "        }\n",
    "\n",
    "        # Initialize XGBoost Classifier\n",
    "        # Use 'objective': 'binary:logistic' for binary classification\n",
    "        # 'eval_metric': 'logloss' or 'auc' are common for evaluation during training\n",
    "        # 'use_label_encoder=False' is recommended to avoid deprecation warnings\n",
    "        xgb_model = xgb.XGBClassifier(objective='binary:logistic', \n",
    "                                      eval_metric='logloss', \n",
    "                                      use_label_encoder=False, \n",
    "                                      random_state=42)\n",
    "\n",
    "        # Initialize GridSearchCV\n",
    "        grid_search_xgb = GridSearchCV(estimator=xgb_model, \n",
    "                                       param_grid=param_grid_xgb, \n",
    "                                       cv=5,                  # 5-fold cross-validation\n",
    "                                       scoring='accuracy',    # Optimize for accuracy\n",
    "                                       n_jobs=-1,             # Use all CPU cores\n",
    "                                       verbose=2)             # Show progress\n",
    "\n",
    "        # Fit GridSearchCV - This might take a while!\n",
    "        print(\"\\nStarting GridSearchCV for XGBoostClassifier...\")\n",
    "        grid_search_xgb.fit(X_train_scaled, y_train_full)\n",
    "\n",
    "        # Get the best parameters and the best score\n",
    "        print(f\"Best parameters found for XGBoost: {grid_search_xgb.best_params_}\")\n",
    "        best_xgb_score = grid_search_xgb.best_score_\n",
    "        print(f\"Best cross-validation accuracy (XGBoost): {best_xgb_score:.4f}\")\n",
    "\n",
    "        # Use the best estimator found by GridSearchCV\n",
    "        best_xgb_model = grid_search_xgb.best_estimator_\n",
    "\n",
    "        # --- Prediction ---\n",
    "        print(\"\\nMaking predictions with the tuned XGBoost model...\")\n",
    "        predictions_xgb = best_xgb_model.predict(X_test_scaled)\n",
    "        # XGBoost predict often returns probabilities by default if not configured, \n",
    "        # but for binary:logistic, .predict() should give 0/1. Let's ensure.\n",
    "        # If it returned probabilities, you'd use: \n",
    "        # predictions_xgb = (best_xgb_model.predict_proba(X_test_scaled)[:,1] >= 0.5).astype(int)\n",
    "\n",
    "        # --- Output Generation ---\n",
    "        output_df_xgb = pd.DataFrame(predictions_xgb, columns=None) # No header, single column\n",
    "        output_filename_xgb = 'answers_xgboost.csv'\n",
    "        try:\n",
    "            output_df_xgb.to_csv(output_filename_xgb, index=False, header=False)\n",
    "            print(f\"\\nXGBoost predictions saved to {output_filename_xgb}\")\n",
    "            \n",
    "            # --- Feedback ---\n",
    "            print(f\"\\nEstimated model accuracy (XGBoost CV): {best_xgb_score:.4f}\")\n",
    "            if best_xgb_score < 0.865:\n",
    "                 print(\"XGBoost CV Accuracy is below the minimum threshold (86.5%).\")\n",
    "            elif best_xgb_score >= 0.865 and best_xgb_score <= 0.915:\n",
    "                 print(\"XGBoost CV Accuracy is within the target range (86.5% - 91.5%). Good!\")\n",
    "            else: # best_xgb_score > 0.915\n",
    "                 print(f\"XGBoost CV Accuracy ({best_xgb_score:.4f}) meets or exceeds the 91.5% target. Excellent!\")\n",
    "                 print(\"This model might achieve the full score.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError saving XGBoost predictions to {output_filename_xgb}: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nError: Training and test feature columns do not match after reloading.\")\n",
    "else:\n",
    "    print(\"\\nError: 'target' column not found in train.csv or columns mismatch after reloading.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a394388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Data loaded successfully.\n",
      "Train and test columns match.\n",
      "Data scaled and split.\n",
      "Train shape: (1030, 16), Validation shape: (258, 16), Test shape: (430, 16)\n",
      "\n",
      "Model Architecture:\n",
      "SimpleNN(\n",
      "  (layer_1): Linear(in_features=16, out_features=128, bias=True)\n",
      "  (relu_1): ReLU()\n",
      "  (dropout_1): Dropout(p=0.3, inplace=False)\n",
      "  (layer_2): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (relu_2): ReLU()\n",
      "  (dropout_2): Dropout(p=0.3, inplace=False)\n",
      "  (layer_4): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (relu_4): ReLU()\n",
      "  (dropout_4): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "Starting Training...\n",
      "Epoch 1/100 | Train Loss: 0.5369 | Train Acc: 0.7971 | Val Loss: 0.3617 | Val Acc: 0.8411\n",
      "Epoch 2/100 | Train Loss: 0.3512 | Train Acc: 0.8612 | Val Loss: 0.3379 | Val Acc: 0.8682\n",
      "Epoch 3/100 | Train Loss: 0.3434 | Train Acc: 0.8573 | Val Loss: 0.3278 | Val Acc: 0.8682\n",
      "Epoch 4/100 | Train Loss: 0.3317 | Train Acc: 0.8602 | Val Loss: 0.3225 | Val Acc: 0.8760\n",
      "Epoch 5/100 | Train Loss: 0.3241 | Train Acc: 0.8670 | Val Loss: 0.3306 | Val Acc: 0.8643\n",
      "Epoch 6/100 | Train Loss: 0.3198 | Train Acc: 0.8660 | Val Loss: 0.3156 | Val Acc: 0.8682\n",
      "Epoch 7/100 | Train Loss: 0.3336 | Train Acc: 0.8573 | Val Loss: 0.3263 | Val Acc: 0.8605\n",
      "Epoch 8/100 | Train Loss: 0.3261 | Train Acc: 0.8709 | Val Loss: 0.3153 | Val Acc: 0.8798\n",
      "Epoch 9/100 | Train Loss: 0.3166 | Train Acc: 0.8709 | Val Loss: 0.3194 | Val Acc: 0.8798\n",
      "Epoch 10/100 | Train Loss: 0.3219 | Train Acc: 0.8757 | Val Loss: 0.3144 | Val Acc: 0.8837\n",
      "Epoch 11/100 | Train Loss: 0.3195 | Train Acc: 0.8660 | Val Loss: 0.3156 | Val Acc: 0.8721\n",
      "Epoch 12/100 | Train Loss: 0.3210 | Train Acc: 0.8699 | Val Loss: 0.3172 | Val Acc: 0.8721\n",
      "Epoch 13/100 | Train Loss: 0.3212 | Train Acc: 0.8689 | Val Loss: 0.3134 | Val Acc: 0.8760\n",
      "Epoch 14/100 | Train Loss: 0.3105 | Train Acc: 0.8728 | Val Loss: 0.3142 | Val Acc: 0.8721\n",
      "Epoch 15/100 | Train Loss: 0.3171 | Train Acc: 0.8650 | Val Loss: 0.3143 | Val Acc: 0.8760\n",
      "Epoch 16/100 | Train Loss: 0.3071 | Train Acc: 0.8738 | Val Loss: 0.3080 | Val Acc: 0.8721\n",
      "Epoch 17/100 | Train Loss: 0.3126 | Train Acc: 0.8699 | Val Loss: 0.3068 | Val Acc: 0.8721\n",
      "Epoch 18/100 | Train Loss: 0.3063 | Train Acc: 0.8641 | Val Loss: 0.3090 | Val Acc: 0.8721\n",
      "Epoch 19/100 | Train Loss: 0.3098 | Train Acc: 0.8699 | Val Loss: 0.3162 | Val Acc: 0.8798\n",
      "Epoch 20/100 | Train Loss: 0.3156 | Train Acc: 0.8660 | Val Loss: 0.3092 | Val Acc: 0.8682\n",
      "Epoch 21/100 | Train Loss: 0.3089 | Train Acc: 0.8718 | Val Loss: 0.3091 | Val Acc: 0.8682\n",
      "Epoch 22/100 | Train Loss: 0.3083 | Train Acc: 0.8786 | Val Loss: 0.3090 | Val Acc: 0.8798\n",
      "Epoch 23/100 | Train Loss: 0.3147 | Train Acc: 0.8660 | Val Loss: 0.3104 | Val Acc: 0.8721\n",
      "Epoch 24/100 | Train Loss: 0.3127 | Train Acc: 0.8680 | Val Loss: 0.3129 | Val Acc: 0.8682\n",
      "Epoch 25/100 | Train Loss: 0.3062 | Train Acc: 0.8621 | Val Loss: 0.3077 | Val Acc: 0.8837\n",
      "Epoch 26/100 | Train Loss: 0.3084 | Train Acc: 0.8728 | Val Loss: 0.3059 | Val Acc: 0.8721\n",
      "Epoch 27/100 | Train Loss: 0.3093 | Train Acc: 0.8689 | Val Loss: 0.3093 | Val Acc: 0.8643\n",
      "Epoch 28/100 | Train Loss: 0.3118 | Train Acc: 0.8709 | Val Loss: 0.3028 | Val Acc: 0.8876\n",
      "Epoch 29/100 | Train Loss: 0.3104 | Train Acc: 0.8689 | Val Loss: 0.3076 | Val Acc: 0.8721\n",
      "Epoch 30/100 | Train Loss: 0.3055 | Train Acc: 0.8718 | Val Loss: 0.3060 | Val Acc: 0.8760\n",
      "Epoch 31/100 | Train Loss: 0.2958 | Train Acc: 0.8777 | Val Loss: 0.3044 | Val Acc: 0.8760\n",
      "Epoch 32/100 | Train Loss: 0.3038 | Train Acc: 0.8718 | Val Loss: 0.3028 | Val Acc: 0.8643\n",
      "Epoch 33/100 | Train Loss: 0.3073 | Train Acc: 0.8660 | Val Loss: 0.3068 | Val Acc: 0.8760\n",
      "Epoch 34/100 | Train Loss: 0.2955 | Train Acc: 0.8816 | Val Loss: 0.3003 | Val Acc: 0.8721\n",
      "Epoch 35/100 | Train Loss: 0.3092 | Train Acc: 0.8748 | Val Loss: 0.3061 | Val Acc: 0.8760\n",
      "Epoch 36/100 | Train Loss: 0.3040 | Train Acc: 0.8718 | Val Loss: 0.2989 | Val Acc: 0.8760\n",
      "Epoch 37/100 | Train Loss: 0.2896 | Train Acc: 0.8728 | Val Loss: 0.2962 | Val Acc: 0.8876\n",
      "Epoch 38/100 | Train Loss: 0.2930 | Train Acc: 0.8796 | Val Loss: 0.3002 | Val Acc: 0.8837\n",
      "Epoch 39/100 | Train Loss: 0.2927 | Train Acc: 0.8796 | Val Loss: 0.2962 | Val Acc: 0.8876\n",
      "Epoch 40/100 | Train Loss: 0.2925 | Train Acc: 0.8845 | Val Loss: 0.2978 | Val Acc: 0.8837\n",
      "Epoch 41/100 | Train Loss: 0.2973 | Train Acc: 0.8767 | Val Loss: 0.2944 | Val Acc: 0.8837\n",
      "Epoch 42/100 | Train Loss: 0.3033 | Train Acc: 0.8699 | Val Loss: 0.2962 | Val Acc: 0.8876\n",
      "Epoch 43/100 | Train Loss: 0.2903 | Train Acc: 0.8796 | Val Loss: 0.2973 | Val Acc: 0.8915\n",
      "Epoch 44/100 | Train Loss: 0.2918 | Train Acc: 0.8816 | Val Loss: 0.2960 | Val Acc: 0.8876\n",
      "Epoch 45/100 | Train Loss: 0.2954 | Train Acc: 0.8709 | Val Loss: 0.2974 | Val Acc: 0.8837\n",
      "Epoch 46/100 | Train Loss: 0.2875 | Train Acc: 0.8845 | Val Loss: 0.2973 | Val Acc: 0.8760\n",
      "Epoch 47/100 | Train Loss: 0.2927 | Train Acc: 0.8796 | Val Loss: 0.2936 | Val Acc: 0.8837\n",
      "Epoch 48/100 | Train Loss: 0.2876 | Train Acc: 0.8786 | Val Loss: 0.2972 | Val Acc: 0.8953\n",
      "Epoch 49/100 | Train Loss: 0.2869 | Train Acc: 0.8816 | Val Loss: 0.2998 | Val Acc: 0.8721\n",
      "Epoch 50/100 | Train Loss: 0.2837 | Train Acc: 0.8806 | Val Loss: 0.2947 | Val Acc: 0.8837\n",
      "Epoch 51/100 | Train Loss: 0.2855 | Train Acc: 0.8816 | Val Loss: 0.2936 | Val Acc: 0.8837\n",
      "Epoch 52/100 | Train Loss: 0.2871 | Train Acc: 0.8728 | Val Loss: 0.2910 | Val Acc: 0.8992\n",
      "Epoch 53/100 | Train Loss: 0.2788 | Train Acc: 0.8893 | Val Loss: 0.2976 | Val Acc: 0.8837\n",
      "Epoch 54/100 | Train Loss: 0.2851 | Train Acc: 0.8893 | Val Loss: 0.2923 | Val Acc: 0.8953\n",
      "Epoch 55/100 | Train Loss: 0.2860 | Train Acc: 0.8806 | Val Loss: 0.2984 | Val Acc: 0.8798\n",
      "Epoch 56/100 | Train Loss: 0.2878 | Train Acc: 0.8854 | Val Loss: 0.2939 | Val Acc: 0.8837\n",
      "Epoch 57/100 | Train Loss: 0.2843 | Train Acc: 0.8845 | Val Loss: 0.3068 | Val Acc: 0.8798\n",
      "Epoch 58/100 | Train Loss: 0.2794 | Train Acc: 0.8874 | Val Loss: 0.2932 | Val Acc: 0.8915\n",
      "Epoch 59/100 | Train Loss: 0.2802 | Train Acc: 0.8806 | Val Loss: 0.2941 | Val Acc: 0.8876\n",
      "Epoch 60/100 | Train Loss: 0.2823 | Train Acc: 0.8864 | Val Loss: 0.2934 | Val Acc: 0.8798\n",
      "Epoch 61/100 | Train Loss: 0.2753 | Train Acc: 0.8825 | Val Loss: 0.2972 | Val Acc: 0.8837\n",
      "Epoch 62/100 | Train Loss: 0.2680 | Train Acc: 0.8932 | Val Loss: 0.2968 | Val Acc: 0.8837\n",
      "Epoch 63/100 | Train Loss: 0.2820 | Train Acc: 0.8728 | Val Loss: 0.2924 | Val Acc: 0.8798\n",
      "Epoch 64/100 | Train Loss: 0.2779 | Train Acc: 0.8825 | Val Loss: 0.2903 | Val Acc: 0.8876\n",
      "Epoch 65/100 | Train Loss: 0.2700 | Train Acc: 0.8951 | Val Loss: 0.2938 | Val Acc: 0.8760\n",
      "Epoch 66/100 | Train Loss: 0.2826 | Train Acc: 0.8845 | Val Loss: 0.2881 | Val Acc: 0.8915\n",
      "Epoch 67/100 | Train Loss: 0.2740 | Train Acc: 0.8835 | Val Loss: 0.2879 | Val Acc: 0.8876\n",
      "Epoch 68/100 | Train Loss: 0.2833 | Train Acc: 0.8835 | Val Loss: 0.2962 | Val Acc: 0.8837\n",
      "Epoch 69/100 | Train Loss: 0.2772 | Train Acc: 0.8835 | Val Loss: 0.2968 | Val Acc: 0.8798\n",
      "Epoch 70/100 | Train Loss: 0.2720 | Train Acc: 0.8903 | Val Loss: 0.2868 | Val Acc: 0.8876\n",
      "Epoch 71/100 | Train Loss: 0.2760 | Train Acc: 0.8883 | Val Loss: 0.2978 | Val Acc: 0.8760\n",
      "Epoch 72/100 | Train Loss: 0.2691 | Train Acc: 0.8864 | Val Loss: 0.2907 | Val Acc: 0.8760\n",
      "Epoch 73/100 | Train Loss: 0.2652 | Train Acc: 0.8864 | Val Loss: 0.2940 | Val Acc: 0.8837\n",
      "Epoch 74/100 | Train Loss: 0.2672 | Train Acc: 0.8951 | Val Loss: 0.2874 | Val Acc: 0.8953\n",
      "Epoch 75/100 | Train Loss: 0.2859 | Train Acc: 0.8903 | Val Loss: 0.3100 | Val Acc: 0.8837\n",
      "Epoch 76/100 | Train Loss: 0.2756 | Train Acc: 0.8854 | Val Loss: 0.2959 | Val Acc: 0.8798\n",
      "Epoch 77/100 | Train Loss: 0.2610 | Train Acc: 0.8932 | Val Loss: 0.3030 | Val Acc: 0.8798\n",
      "Epoch 78/100 | Train Loss: 0.2776 | Train Acc: 0.8825 | Val Loss: 0.3021 | Val Acc: 0.8876\n",
      "Epoch 79/100 | Train Loss: 0.2719 | Train Acc: 0.8922 | Val Loss: 0.2992 | Val Acc: 0.8953\n",
      "Epoch 80/100 | Train Loss: 0.2686 | Train Acc: 0.8913 | Val Loss: 0.2952 | Val Acc: 0.8837\n",
      "Epoch 81/100 | Train Loss: 0.2675 | Train Acc: 0.8874 | Val Loss: 0.3022 | Val Acc: 0.8915\n",
      "Epoch 82/100 | Train Loss: 0.2645 | Train Acc: 0.8903 | Val Loss: 0.2905 | Val Acc: 0.8915\n",
      "Epoch 83/100 | Train Loss: 0.2624 | Train Acc: 0.8913 | Val Loss: 0.2915 | Val Acc: 0.8760\n",
      "Epoch 84/100 | Train Loss: 0.2516 | Train Acc: 0.8981 | Val Loss: 0.2971 | Val Acc: 0.8876\n",
      "Epoch 85/100 | Train Loss: 0.2706 | Train Acc: 0.8932 | Val Loss: 0.2954 | Val Acc: 0.8876\n",
      "Epoch 86/100 | Train Loss: 0.2562 | Train Acc: 0.8942 | Val Loss: 0.3018 | Val Acc: 0.8760\n",
      "Epoch 87/100 | Train Loss: 0.2601 | Train Acc: 0.9000 | Val Loss: 0.2979 | Val Acc: 0.8837\n",
      "Epoch 88/100 | Train Loss: 0.2573 | Train Acc: 0.9000 | Val Loss: 0.2957 | Val Acc: 0.8915\n",
      "Epoch 89/100 | Train Loss: 0.2624 | Train Acc: 0.8893 | Val Loss: 0.2864 | Val Acc: 0.8876\n",
      "Epoch 90/100 | Train Loss: 0.2534 | Train Acc: 0.8932 | Val Loss: 0.2905 | Val Acc: 0.8876\n",
      "Epoch 91/100 | Train Loss: 0.2547 | Train Acc: 0.9000 | Val Loss: 0.3085 | Val Acc: 0.8876\n",
      "Epoch 92/100 | Train Loss: 0.2575 | Train Acc: 0.8951 | Val Loss: 0.2967 | Val Acc: 0.8837\n",
      "Epoch 93/100 | Train Loss: 0.2534 | Train Acc: 0.8951 | Val Loss: 0.2901 | Val Acc: 0.8915\n",
      "Epoch 94/100 | Train Loss: 0.2532 | Train Acc: 0.8893 | Val Loss: 0.2981 | Val Acc: 0.8953\n",
      "Epoch 95/100 | Train Loss: 0.2526 | Train Acc: 0.8990 | Val Loss: 0.2984 | Val Acc: 0.8798\n",
      "Epoch 96/100 | Train Loss: 0.2494 | Train Acc: 0.8981 | Val Loss: 0.3037 | Val Acc: 0.8992\n",
      "Epoch 97/100 | Train Loss: 0.2514 | Train Acc: 0.8990 | Val Loss: 0.2967 | Val Acc: 0.8837\n",
      "Epoch 98/100 | Train Loss: 0.2492 | Train Acc: 0.9068 | Val Loss: 0.3096 | Val Acc: 0.8721\n",
      "Epoch 99/100 | Train Loss: 0.2535 | Train Acc: 0.9029 | Val Loss: 0.3033 | Val Acc: 0.8915\n",
      "Epoch 100/100 | Train Loss: 0.2432 | Train Acc: 0.9058 | Val Loss: 0.3062 | Val Acc: 0.8915\n",
      "\n",
      "Training Finished.\n",
      "Best Validation Accuracy during training: 0.8992\n",
      "\n",
      "Making predictions with the Neural Network model...\n",
      "\n",
      "Neural Network predictions saved to answers_nn.csv\n",
      "\n",
      "Estimated model accuracy (NN Best Validation): 0.8992\n",
      "NN Validation Accuracy is within the target range (86.5% - 91.5%). Good!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore') # Suppress warnings for cleaner output\n",
    "\n",
    "# --- Configuration ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100 # Increased epochs, consider adding early stopping\n",
    "LEARNING_RATE = 0.001\n",
    "TEST_SPLIT_SIZE = 0.2 # Hold out 20% of training data for validation\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# --- Set Seed for Reproducibility ---\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv('train.csv')\n",
    "    test_df = pd.read_csv('test.csv')\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: train.csv or test.csv not found.\")\n",
    "    exit()\n",
    "\n",
    "# --- Prepare Data ---\n",
    "if 'target' in train_df.columns and all(col in train_df.columns for col in test_df.columns):\n",
    "    X = train_df.drop('target', axis=1)\n",
    "    y = train_df['target']\n",
    "    X_test_final = test_df\n",
    "\n",
    "    if list(X.columns) == list(X_test_final.columns):\n",
    "        print(\"Train and test columns match.\")\n",
    "        \n",
    "        # --- Preprocessing (Scaling) ---\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        X_test_scaled = scaler.transform(X_test_final) \n",
    "        \n",
    "        # --- Train/Validation Split ---\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_scaled, y.values, test_size=TEST_SPLIT_SIZE, random_state=RANDOM_SEED, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"Data scaled and split.\")\n",
    "        print(f\"Train shape: {X_train.shape}, Validation shape: {X_val.shape}, Test shape: {X_test_scaled.shape}\")\n",
    "\n",
    "        # --- PyTorch Dataset ---\n",
    "        class DroneDataset(Dataset):\n",
    "            def __init__(self, features, labels=None):\n",
    "                # Convert to float32, the default float type for PyTorch\n",
    "                self.features = torch.tensor(features, dtype=torch.float32)\n",
    "                # Ensure labels are also float32 for BCELoss and reshape to [n_samples, 1]\n",
    "                self.labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(1) if labels is not None else None\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.features)\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                if self.labels is not None:\n",
    "                    return self.features[idx], self.labels[idx]\n",
    "                else:\n",
    "                    # Return only features if labels are not provided (for test set)\n",
    "                    return self.features[idx]\n",
    "\n",
    "        # Create Datasets\n",
    "        train_dataset = DroneDataset(X_train, y_train)\n",
    "        val_dataset = DroneDataset(X_val, y_val)\n",
    "        test_dataset = DroneDataset(X_test_scaled) # No labels for the final test set\n",
    "\n",
    "        # Create DataLoaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False) # No need to shuffle test data\n",
    "\n",
    "        # --- Neural Network Model ---\n",
    "        class SimpleNN(nn.Module):\n",
    "            def __init__(self, input_dim):\n",
    "                super(SimpleNN, self).__init__()\n",
    "                self.layer_1 = nn.Linear(input_dim, 128) # First hidden layer\n",
    "                self.relu_1 = nn.ReLU()\n",
    "                self.dropout_1 = nn.Dropout(0.3) # Add dropout for regularization\n",
    "                self.layer_2 = nn.Linear(128, 256) \n",
    "                self.relu_2 = nn.ReLU()\n",
    "                self.dropout_2 = nn.Dropout(0.3) # Add dropout for regularization\n",
    "                self.layer_4 = nn.Linear(256, 64)   # Second hidden layer\n",
    "                self.relu_4 = nn.ReLU()\n",
    "                self.dropout_4 = nn.Dropout(0.2) # Add dropout\n",
    "                self.output_layer = nn.Linear(64, 1) # Output layer (1 neuron for binary classification)\n",
    "                self.sigmoid = nn.Sigmoid()         # Sigmoid activation for probability output\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = self.layer_1(x)\n",
    "                x = self.relu_1(x)\n",
    "                x = self.dropout_1(x)\n",
    "                x = self.layer_2(x)\n",
    "                x = self.relu_2(x)\n",
    "                x = self.dropout_2(x)\n",
    "                x = self.layer_4(x)\n",
    "                x = self.relu_4(x)\n",
    "                x = self.dropout_4(x)\n",
    "                x = self.output_layer(x)\n",
    "                x = self.sigmoid(x)\n",
    "                return x\n",
    "\n",
    "        # Instantiate the model\n",
    "        input_dimension = X_train.shape[1] # Number of features\n",
    "        model = SimpleNN(input_dimension).to(DEVICE)\n",
    "        print(\"\\nModel Architecture:\")\n",
    "        print(model)\n",
    "\n",
    "        # --- Loss Function and Optimizer ---\n",
    "        criterion = nn.BCELoss() # Binary Cross Entropy Loss for binary classification\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        # --- Training Loop ---\n",
    "        best_val_accuracy = 0.0\n",
    "        print(\"\\nStarting Training...\")\n",
    "        for epoch in range(EPOCHS):\n",
    "            model.train() # Set model to training mode\n",
    "            train_loss = 0.0\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "            \n",
    "            for features, labels in train_loader:\n",
    "                features, labels = features.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "                # Zero gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Track loss and accuracy\n",
    "                train_loss += loss.item() * features.size(0)\n",
    "                predicted = (outputs > 0.5).float() # Convert probabilities to 0/1\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "            epoch_train_loss = train_loss / len(train_loader.dataset)\n",
    "            epoch_train_acc = correct_train / total_train\n",
    "\n",
    "            # --- Validation Step ---\n",
    "            model.eval() # Set model to evaluation mode\n",
    "            val_loss = 0.0\n",
    "            correct_val = 0\n",
    "            total_val = 0\n",
    "            with torch.no_grad(): # Disable gradient calculation for validation\n",
    "                for features, labels in val_loader:\n",
    "                    features, labels = features.to(DEVICE), labels.to(DEVICE)\n",
    "                    outputs = model(features)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_loss += loss.item() * features.size(0)\n",
    "                    predicted = (outputs > 0.5).float()\n",
    "                    total_val += labels.size(0)\n",
    "                    correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "            epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "            epoch_val_acc = correct_val / total_val\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f} | Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.4f}\")\n",
    "\n",
    "            # Simple check to save the best model based on validation accuracy\n",
    "            # In practice, you might want early stopping based on val_loss\n",
    "            if epoch_val_acc > best_val_accuracy:\n",
    "                best_val_accuracy = epoch_val_acc\n",
    "                # Optional: Save the best model state\n",
    "                # torch.save(model.state_dict(), 'best_nn_model.pth')\n",
    "                # print(f\"--- Best validation accuracy improved to {best_val_accuracy:.4f} ---\")\n",
    "\n",
    "\n",
    "        print(\"\\nTraining Finished.\")\n",
    "        print(f\"Best Validation Accuracy during training: {best_val_accuracy:.4f}\")\n",
    "\n",
    "        # --- Prediction on Final Test Set ---\n",
    "        model.eval() # Ensure model is in evaluation mode\n",
    "        all_predictions = []\n",
    "        with torch.no_grad():\n",
    "            for features in test_loader:\n",
    "                features = features.to(DEVICE)\n",
    "                outputs = model(features)\n",
    "                predicted_probs = outputs.cpu().numpy() # Move to CPU and convert to numpy\n",
    "                predicted_classes = (predicted_probs > 0.5).astype(int) # Threshold at 0.5\n",
    "                all_predictions.extend(predicted_classes.flatten()) # Flatten in case of single-column output\n",
    "\n",
    "        print(\"\\nMaking predictions with the Neural Network model...\")\n",
    "        \n",
    "        # --- Output Generation ---\n",
    "        output_df_nn = pd.DataFrame(all_predictions, columns=None)\n",
    "        output_filename_nn = 'answers_nn.csv'\n",
    "        try:\n",
    "            output_df_nn.to_csv(output_filename_nn, index=False, header=False)\n",
    "            print(f\"\\nNeural Network predictions saved to {output_filename_nn}\")\n",
    "\n",
    "            # --- Feedback based on best validation accuracy ---\n",
    "            print(f\"\\nEstimated model accuracy (NN Best Validation): {best_val_accuracy:.4f}\")\n",
    "            if best_val_accuracy < 0.865:\n",
    "                 print(\"NN Validation Accuracy is below the minimum threshold (86.5%).\")\n",
    "            elif best_val_accuracy >= 0.865 and best_val_accuracy <= 0.915:\n",
    "                 print(\"NN Validation Accuracy is within the target range (86.5% - 91.5%). Good!\")\n",
    "            else: # best_val_accuracy > 0.915\n",
    "                 print(f\"NN Validation Accuracy ({best_val_accuracy:.4f}) meets or exceeds the 91.5% target. Potentially Excellent!\")\n",
    "                 print(\"This model might achieve the full score. Submit answers_nn.csv to check.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError saving Neural Network predictions to {output_filename_nn}: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nError: Training and test feature columns do not match.\")\n",
    "else:\n",
    "    print(\"\\nError: 'target' column not found in train.csv or columns mismatch.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608e6f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Data loaded successfully.\n",
      "Train and test columns match.\n",
      "Data scaled and split.\n",
      "Train shape: (1030, 16), Validation shape: (258, 16), Test shape: (430, 16)\n",
      "Model Architecture:\n",
      "SimpleNN(\n",
      "  (layer_1): Linear(in_features=16, out_features=256, bias=True)\n",
      "  (relu_1): ReLU()\n",
      "  (dropout_1): Dropout(p=0.4, inplace=False)\n",
      "  (layer_2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (relu_2): ReLU()\n",
      "  (dropout_2): Dropout(p=0.4, inplace=False)\n",
      "  (layer_3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (relu_3): ReLU()\n",
      "  (dropout_3): Dropout(p=0.3, inplace=False)\n",
      "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Starting Training...\n",
      "Epoch 1/500 | Train Loss: 0.5806 | Train Acc: 0.7845 | Val Loss: 0.3965 | Val Acc: 0.8411\n",
      "Epoch 2/500 | Train Loss: 0.3690 | Train Acc: 0.8505 | Val Loss: 0.3510 | Val Acc: 0.8527\n",
      "Epoch 3/500 | Train Loss: 0.3416 | Train Acc: 0.8621 | Val Loss: 0.3331 | Val Acc: 0.8566\n",
      "Epoch 4/500 | Train Loss: 0.3399 | Train Acc: 0.8641 | Val Loss: 0.3286 | Val Acc: 0.8643\n",
      "Epoch 5/500 | Train Loss: 0.3284 | Train Acc: 0.8670 | Val Loss: 0.3268 | Val Acc: 0.8643\n",
      "Epoch 6/500 | Train Loss: 0.3331 | Train Acc: 0.8573 | Val Loss: 0.3243 | Val Acc: 0.8721\n",
      "Epoch 7/500 | Train Loss: 0.3308 | Train Acc: 0.8592 | Val Loss: 0.3225 | Val Acc: 0.8721\n",
      "Epoch 8/500 | Train Loss: 0.3354 | Train Acc: 0.8631 | Val Loss: 0.3258 | Val Acc: 0.8643\n",
      "Epoch 9/500 | Train Loss: 0.3247 | Train Acc: 0.8641 | Val Loss: 0.3185 | Val Acc: 0.8721\n",
      "Epoch 10/500 | Train Loss: 0.3202 | Train Acc: 0.8709 | Val Loss: 0.3214 | Val Acc: 0.8566\n",
      "Epoch 11/500 | Train Loss: 0.3188 | Train Acc: 0.8670 | Val Loss: 0.3162 | Val Acc: 0.8643\n",
      "Epoch 12/500 | Train Loss: 0.3252 | Train Acc: 0.8680 | Val Loss: 0.3170 | Val Acc: 0.8760\n",
      "Epoch 13/500 | Train Loss: 0.3228 | Train Acc: 0.8689 | Val Loss: 0.3115 | Val Acc: 0.8837\n",
      "Epoch 14/500 | Train Loss: 0.3129 | Train Acc: 0.8670 | Val Loss: 0.3176 | Val Acc: 0.8682\n",
      "Epoch 15/500 | Train Loss: 0.3206 | Train Acc: 0.8718 | Val Loss: 0.3115 | Val Acc: 0.8760\n",
      "Epoch 16/500 | Train Loss: 0.3146 | Train Acc: 0.8709 | Val Loss: 0.3169 | Val Acc: 0.8605\n",
      "Epoch 17/500 | Train Loss: 0.3160 | Train Acc: 0.8641 | Val Loss: 0.3112 | Val Acc: 0.8760\n",
      "Epoch 18/500 | Train Loss: 0.3165 | Train Acc: 0.8699 | Val Loss: 0.3147 | Val Acc: 0.8682\n",
      "Epoch 19/500 | Train Loss: 0.3121 | Train Acc: 0.8709 | Val Loss: 0.3130 | Val Acc: 0.8721\n",
      "Epoch 20/500 | Train Loss: 0.3097 | Train Acc: 0.8748 | Val Loss: 0.3143 | Val Acc: 0.8682\n",
      "Epoch 21/500 | Train Loss: 0.3167 | Train Acc: 0.8699 | Val Loss: 0.3161 | Val Acc: 0.8721\n",
      "Epoch 22/500 | Train Loss: 0.3084 | Train Acc: 0.8738 | Val Loss: 0.3181 | Val Acc: 0.8721\n",
      "Epoch 23/500 | Train Loss: 0.3156 | Train Acc: 0.8670 | Val Loss: 0.3195 | Val Acc: 0.8605\n",
      "Epoch 24/500 | Train Loss: 0.3176 | Train Acc: 0.8650 | Val Loss: 0.3170 | Val Acc: 0.8605\n",
      "Epoch 25/500 | Train Loss: 0.3150 | Train Acc: 0.8738 | Val Loss: 0.3142 | Val Acc: 0.8682\n",
      "Epoch 26/500 | Train Loss: 0.3129 | Train Acc: 0.8660 | Val Loss: 0.3160 | Val Acc: 0.8682\n",
      "Epoch 27/500 | Train Loss: 0.3147 | Train Acc: 0.8680 | Val Loss: 0.3192 | Val Acc: 0.8605\n",
      "Epoch 28/500 | Train Loss: 0.3111 | Train Acc: 0.8738 | Val Loss: 0.3169 | Val Acc: 0.8643\n",
      "Epoch 29/500 | Train Loss: 0.3154 | Train Acc: 0.8709 | Val Loss: 0.3127 | Val Acc: 0.8721\n",
      "Epoch 30/500 | Train Loss: 0.3135 | Train Acc: 0.8709 | Val Loss: 0.3118 | Val Acc: 0.8682\n",
      "Epoch 31/500 | Train Loss: 0.3133 | Train Acc: 0.8718 | Val Loss: 0.3129 | Val Acc: 0.8682\n",
      "Epoch 32/500 | Train Loss: 0.3085 | Train Acc: 0.8738 | Val Loss: 0.3133 | Val Acc: 0.8721\n",
      "Epoch 33/500 | Train Loss: 0.3141 | Train Acc: 0.8689 | Val Loss: 0.3160 | Val Acc: 0.8682\n",
      "Epoch 34/500 | Train Loss: 0.2990 | Train Acc: 0.8777 | Val Loss: 0.3125 | Val Acc: 0.8682\n",
      "Epoch 35/500 | Train Loss: 0.3109 | Train Acc: 0.8670 | Val Loss: 0.3143 | Val Acc: 0.8721\n",
      "Epoch 36/500 | Train Loss: 0.2961 | Train Acc: 0.8786 | Val Loss: 0.3113 | Val Acc: 0.8760\n",
      "Epoch 37/500 | Train Loss: 0.2951 | Train Acc: 0.8738 | Val Loss: 0.3122 | Val Acc: 0.8721\n",
      "Epoch 38/500 | Train Loss: 0.3064 | Train Acc: 0.8718 | Val Loss: 0.3138 | Val Acc: 0.8721\n",
      "Epoch 39/500 | Train Loss: 0.3062 | Train Acc: 0.8786 | Val Loss: 0.3123 | Val Acc: 0.8760\n",
      "Epoch 40/500 | Train Loss: 0.2981 | Train Acc: 0.8738 | Val Loss: 0.3111 | Val Acc: 0.8798\n",
      "Epoch 41/500 | Train Loss: 0.2990 | Train Acc: 0.8660 | Val Loss: 0.3090 | Val Acc: 0.8760\n",
      "Epoch 42/500 | Train Loss: 0.3058 | Train Acc: 0.8699 | Val Loss: 0.3074 | Val Acc: 0.8760\n",
      "Epoch 43/500 | Train Loss: 0.3006 | Train Acc: 0.8738 | Val Loss: 0.3070 | Val Acc: 0.8760\n",
      "Epoch 44/500 | Train Loss: 0.3069 | Train Acc: 0.8767 | Val Loss: 0.3086 | Val Acc: 0.8760\n",
      "Epoch 45/500 | Train Loss: 0.3052 | Train Acc: 0.8757 | Val Loss: 0.3090 | Val Acc: 0.8837\n",
      "Epoch 46/500 | Train Loss: 0.2948 | Train Acc: 0.8796 | Val Loss: 0.3145 | Val Acc: 0.8760\n",
      "Epoch 47/500 | Train Loss: 0.3014 | Train Acc: 0.8680 | Val Loss: 0.3055 | Val Acc: 0.8798\n",
      "Epoch 48/500 | Train Loss: 0.2975 | Train Acc: 0.8796 | Val Loss: 0.3061 | Val Acc: 0.8837\n",
      "Epoch 49/500 | Train Loss: 0.2944 | Train Acc: 0.8825 | Val Loss: 0.3040 | Val Acc: 0.8798\n",
      "Epoch 50/500 | Train Loss: 0.2962 | Train Acc: 0.8718 | Val Loss: 0.2999 | Val Acc: 0.8798\n",
      "Epoch 51/500 | Train Loss: 0.2881 | Train Acc: 0.8757 | Val Loss: 0.3038 | Val Acc: 0.8798\n",
      "Epoch 52/500 | Train Loss: 0.2908 | Train Acc: 0.8786 | Val Loss: 0.3043 | Val Acc: 0.8798\n",
      "Epoch 53/500 | Train Loss: 0.2909 | Train Acc: 0.8806 | Val Loss: 0.3073 | Val Acc: 0.8760\n",
      "Epoch 54/500 | Train Loss: 0.2906 | Train Acc: 0.8748 | Val Loss: 0.3097 | Val Acc: 0.8798\n",
      "Epoch 55/500 | Train Loss: 0.2871 | Train Acc: 0.8728 | Val Loss: 0.3034 | Val Acc: 0.8682\n",
      "Epoch 56/500 | Train Loss: 0.2854 | Train Acc: 0.8864 | Val Loss: 0.3048 | Val Acc: 0.8798\n",
      "Epoch 57/500 | Train Loss: 0.2902 | Train Acc: 0.8767 | Val Loss: 0.3075 | Val Acc: 0.8837\n",
      "Epoch 58/500 | Train Loss: 0.2926 | Train Acc: 0.8786 | Val Loss: 0.3091 | Val Acc: 0.8837\n",
      "Epoch 59/500 | Train Loss: 0.2898 | Train Acc: 0.8825 | Val Loss: 0.3134 | Val Acc: 0.8682\n",
      "Epoch 60/500 | Train Loss: 0.2911 | Train Acc: 0.8767 | Val Loss: 0.3051 | Val Acc: 0.8876\n",
      "Epoch 61/500 | Train Loss: 0.2948 | Train Acc: 0.8835 | Val Loss: 0.3076 | Val Acc: 0.8643\n",
      "Epoch 62/500 | Train Loss: 0.2886 | Train Acc: 0.8786 | Val Loss: 0.3028 | Val Acc: 0.8798\n",
      "Epoch 63/500 | Train Loss: 0.2787 | Train Acc: 0.8854 | Val Loss: 0.3057 | Val Acc: 0.8760\n",
      "Epoch 64/500 | Train Loss: 0.2859 | Train Acc: 0.8835 | Val Loss: 0.3003 | Val Acc: 0.8837\n",
      "Epoch 65/500 | Train Loss: 0.2818 | Train Acc: 0.8728 | Val Loss: 0.3010 | Val Acc: 0.8798\n",
      "Epoch 66/500 | Train Loss: 0.2865 | Train Acc: 0.8738 | Val Loss: 0.3037 | Val Acc: 0.8953\n",
      "Epoch 67/500 | Train Loss: 0.2831 | Train Acc: 0.8796 | Val Loss: 0.3029 | Val Acc: 0.8837\n",
      "Epoch 68/500 | Train Loss: 0.2860 | Train Acc: 0.8835 | Val Loss: 0.3028 | Val Acc: 0.8760\n",
      "Epoch 69/500 | Train Loss: 0.2707 | Train Acc: 0.8806 | Val Loss: 0.2976 | Val Acc: 0.8798\n",
      "Epoch 70/500 | Train Loss: 0.2729 | Train Acc: 0.8971 | Val Loss: 0.3009 | Val Acc: 0.8798\n",
      "Epoch 71/500 | Train Loss: 0.2791 | Train Acc: 0.8893 | Val Loss: 0.2925 | Val Acc: 0.8837\n",
      "Epoch 72/500 | Train Loss: 0.2840 | Train Acc: 0.8835 | Val Loss: 0.2940 | Val Acc: 0.8798\n",
      "Epoch 73/500 | Train Loss: 0.2651 | Train Acc: 0.8942 | Val Loss: 0.2976 | Val Acc: 0.8760\n",
      "Epoch 74/500 | Train Loss: 0.2711 | Train Acc: 0.8883 | Val Loss: 0.3014 | Val Acc: 0.8760\n",
      "Epoch 75/500 | Train Loss: 0.2784 | Train Acc: 0.8835 | Val Loss: 0.3031 | Val Acc: 0.8837\n",
      "Epoch 76/500 | Train Loss: 0.2886 | Train Acc: 0.8825 | Val Loss: 0.3018 | Val Acc: 0.8915\n",
      "Epoch 77/500 | Train Loss: 0.2800 | Train Acc: 0.8806 | Val Loss: 0.3037 | Val Acc: 0.8721\n",
      "Epoch 78/500 | Train Loss: 0.2832 | Train Acc: 0.8816 | Val Loss: 0.2949 | Val Acc: 0.8760\n",
      "Epoch 79/500 | Train Loss: 0.2785 | Train Acc: 0.8932 | Val Loss: 0.2967 | Val Acc: 0.8837\n",
      "Epoch 80/500 | Train Loss: 0.2700 | Train Acc: 0.8854 | Val Loss: 0.3020 | Val Acc: 0.8915\n",
      "Epoch 81/500 | Train Loss: 0.2557 | Train Acc: 0.8883 | Val Loss: 0.3058 | Val Acc: 0.8953\n",
      "Epoch 82/500 | Train Loss: 0.2850 | Train Acc: 0.8825 | Val Loss: 0.3063 | Val Acc: 0.8953\n",
      "Epoch 83/500 | Train Loss: 0.2941 | Train Acc: 0.8845 | Val Loss: 0.2985 | Val Acc: 0.8798\n",
      "Epoch 84/500 | Train Loss: 0.2681 | Train Acc: 0.8903 | Val Loss: 0.3057 | Val Acc: 0.8837\n",
      "Epoch 85/500 | Train Loss: 0.2820 | Train Acc: 0.8748 | Val Loss: 0.3001 | Val Acc: 0.8915\n",
      "Epoch 86/500 | Train Loss: 0.2698 | Train Acc: 0.8786 | Val Loss: 0.3008 | Val Acc: 0.8837\n",
      "Epoch 87/500 | Train Loss: 0.2738 | Train Acc: 0.8893 | Val Loss: 0.3007 | Val Acc: 0.8760\n",
      "Epoch 88/500 | Train Loss: 0.2623 | Train Acc: 0.8990 | Val Loss: 0.2986 | Val Acc: 0.8953\n",
      "Epoch 89/500 | Train Loss: 0.2767 | Train Acc: 0.8835 | Val Loss: 0.2983 | Val Acc: 0.8721\n",
      "Epoch 90/500 | Train Loss: 0.2771 | Train Acc: 0.8816 | Val Loss: 0.2991 | Val Acc: 0.8721\n",
      "Epoch 91/500 | Train Loss: 0.2768 | Train Acc: 0.8835 | Val Loss: 0.2962 | Val Acc: 0.8915\n",
      "Epoch 92/500 | Train Loss: 0.2698 | Train Acc: 0.8874 | Val Loss: 0.3009 | Val Acc: 0.8837\n",
      "Epoch 93/500 | Train Loss: 0.2809 | Train Acc: 0.8951 | Val Loss: 0.3022 | Val Acc: 0.8798\n",
      "Epoch 94/500 | Train Loss: 0.2731 | Train Acc: 0.8913 | Val Loss: 0.2999 | Val Acc: 0.8721\n",
      "Epoch 95/500 | Train Loss: 0.2645 | Train Acc: 0.8903 | Val Loss: 0.3029 | Val Acc: 0.8760\n",
      "Epoch 96/500 | Train Loss: 0.2623 | Train Acc: 0.8922 | Val Loss: 0.3027 | Val Acc: 0.8837\n",
      "Epoch 97/500 | Train Loss: 0.2606 | Train Acc: 0.8922 | Val Loss: 0.3019 | Val Acc: 0.8953\n",
      "Epoch 98/500 | Train Loss: 0.2689 | Train Acc: 0.8961 | Val Loss: 0.3043 | Val Acc: 0.8798\n",
      "Epoch 99/500 | Train Loss: 0.2611 | Train Acc: 0.8932 | Val Loss: 0.3081 | Val Acc: 0.8798\n",
      "Epoch 100/500 | Train Loss: 0.2559 | Train Acc: 0.8981 | Val Loss: 0.3107 | Val Acc: 0.8876\n",
      "Epoch 101/500 | Train Loss: 0.2696 | Train Acc: 0.8874 | Val Loss: 0.3161 | Val Acc: 0.8798\n",
      "Epoch 102/500 | Train Loss: 0.2571 | Train Acc: 0.8961 | Val Loss: 0.3085 | Val Acc: 0.8798\n",
      "Epoch 103/500 | Train Loss: 0.2595 | Train Acc: 0.8942 | Val Loss: 0.3051 | Val Acc: 0.8953\n",
      "Epoch 104/500 | Train Loss: 0.2586 | Train Acc: 0.8874 | Val Loss: 0.3078 | Val Acc: 0.9031\n",
      "Epoch 105/500 | Train Loss: 0.2599 | Train Acc: 0.9000 | Val Loss: 0.3096 | Val Acc: 0.8953\n",
      "Epoch 106/500 | Train Loss: 0.2538 | Train Acc: 0.9049 | Val Loss: 0.3104 | Val Acc: 0.8876\n",
      "Epoch 107/500 | Train Loss: 0.2468 | Train Acc: 0.8990 | Val Loss: 0.3084 | Val Acc: 0.8915\n",
      "Epoch 108/500 | Train Loss: 0.2524 | Train Acc: 0.8971 | Val Loss: 0.3068 | Val Acc: 0.8798\n",
      "Epoch 109/500 | Train Loss: 0.2621 | Train Acc: 0.8835 | Val Loss: 0.3017 | Val Acc: 0.8798\n",
      "Epoch 110/500 | Train Loss: 0.2605 | Train Acc: 0.8845 | Val Loss: 0.3042 | Val Acc: 0.8876\n",
      "Epoch 111/500 | Train Loss: 0.2546 | Train Acc: 0.8932 | Val Loss: 0.3076 | Val Acc: 0.8837\n",
      "Early stopping triggered after 40 epochs with no improvement in validation loss.\n",
      "Training Finished.\n",
      "Loaded best model state based on validation loss.\n",
      "Making predictions with the Neural Network model...\n",
      "Neural Network predictions saved to answers.csv\n",
      "Validation metrics from the epoch with the best validation loss:\n",
      "Best Validation Loss achieved: 0.2925\n",
      "Goal is to achieve accuracy between 86.5% and 91.5% on the unseen test set.\n",
      "Your previous score suggests you are very close to the target range.\n",
      "Try submitting the 'answers.csv' file generated by this code.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau # Import Learning Rate Scheduler\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore') # Suppress warnings for cleaner output\n",
    "import copy # To save the best model state\n",
    "\n",
    "# --- Configuration ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 500 # Increased epochs significantly, relying on early stopping\n",
    "LEARNING_RATE = 0.001\n",
    "TEST_SPLIT_SIZE = 0.2 # Hold out 20% of training data for validation\n",
    "RANDOM_SEED = 42\n",
    "EARLY_STOPPING_PATIENCE = 40 # Number of epochs to wait for validation loss improvement\n",
    "LR_SCHEDULER_PATIENCE = 30 # Number of epochs with no improvement after which learning rate will be reduced\n",
    "LR_SCHEDULER_FACTOR = 0.5 # Factor by which the learning rate will be reduced\n",
    "\n",
    "# --- Set Seed for Reproducibility ---\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv('train.csv')\n",
    "    test_df = pd.read_csv('test.csv')\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: train.csv or test.csv not found.\")\n",
    "    exit()\n",
    "\n",
    "# --- Prepare Data ---\n",
    "if 'target' in train_df.columns and all(col in train_df.columns for col in test_df.columns):\n",
    "    X = train_df.drop('target', axis=1)\n",
    "    y = train_df['target']\n",
    "    X_test_final = test_df\n",
    "\n",
    "    if list(X.columns) == list(X_test_final.columns):\n",
    "        print(\"Train and test columns match.\")\n",
    "        \n",
    "        # --- Preprocessing (Scaling) ---\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        X_test_scaled = scaler.transform(X_test_final) \n",
    "        \n",
    "        # --- Train/Validation Split ---\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_scaled, y.values, test_size=TEST_SPLIT_SIZE, random_state=RANDOM_SEED, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"Data scaled and split.\")\n",
    "        print(f\"Train shape: {X_train.shape}, Validation shape: {X_val.shape}, Test shape: {X_test_scaled.shape}\")\n",
    "\n",
    "        # --- PyTorch Dataset ---\n",
    "        class DroneDataset(Dataset):\n",
    "            def __init__(self, features, labels=None):\n",
    "                # Convert to float32, the default float type for PyTorch\n",
    "                self.features = torch.tensor(features, dtype=torch.float32)\n",
    "                # Ensure labels are also float32 for BCELoss and reshape to [n_samples, 1]\n",
    "                self.labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(1) if labels is not None else None\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.features)\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                if self.labels is not None:\n",
    "                    return self.features[idx], self.labels[idx]\n",
    "                else:\n",
    "                    # Return only features if labels are not provided (for test set)\n",
    "                    return self.features[idx]\n",
    "\n",
    "        # Create Datasets\n",
    "        train_dataset = DroneDataset(X_train, y_train)\n",
    "        val_dataset = DroneDataset(X_val, y_val)\n",
    "        test_dataset = DroneDataset(X_test_scaled) # No labels for the final test set\n",
    "\n",
    "        # Create DataLoaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False) # No need to shuffle test data\n",
    "\n",
    "        # --- Neural Network Model ---\n",
    "        class SimpleNN(nn.Module):\n",
    "            def __init__(self, input_dim):\n",
    "                super(SimpleNN, self).__init__()\n",
    "                # Slightly increased neurons and added one more layer\n",
    "                self.layer_1 = nn.Linear(input_dim, 256) # First hidden layer\n",
    "                self.relu_1 = nn.ReLU()\n",
    "                self.dropout_1 = nn.Dropout(0.4) # Slightly increased dropout\n",
    "                self.layer_2 = nn.Linear(256, 128)   # Second hidden layer\n",
    "                self.relu_2 = nn.ReLU()\n",
    "                self.dropout_2 = nn.Dropout(0.4) # Slightly increased dropout\n",
    "                self.layer_3 = nn.Linear(128, 64)   # Third hidden layer\n",
    "                self.relu_3 = nn.ReLU()\n",
    "                self.dropout_3 = nn.Dropout(0.3) # Dropout for the new layer\n",
    "                self.output_layer = nn.Linear(64, 1) # Output layer\n",
    "                self.sigmoid = nn.Sigmoid()         # Sigmoid activation\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = self.layer_1(x)\n",
    "                x = self.relu_1(x)\n",
    "                x = self.dropout_1(x)\n",
    "                x = self.layer_2(x)\n",
    "                x = self.relu_2(x)\n",
    "                x = self.dropout_2(x)\n",
    "                x = self.layer_3(x)\n",
    "                x = self.relu_3(x)\n",
    "                x = self.dropout_3(x)\n",
    "                x = self.output_layer(x)\n",
    "                x = self.sigmoid(x)\n",
    "                return x\n",
    "\n",
    "        # Instantiate the model\n",
    "        input_dimension = X_train.shape[1] # Number of features\n",
    "        model = SimpleNN(input_dimension).to(DEVICE)\n",
    "        print(\"Model Architecture:\")\n",
    "        print(model)\n",
    "\n",
    "        # --- Loss Function and Optimizer ---\n",
    "        criterion = nn.BCELoss() # Binary Cross Entropy Loss\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        # Learning Rate Scheduler\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=LR_SCHEDULER_FACTOR, patience=LR_SCHEDULER_PATIENCE, verbose=True)\n",
    "\n",
    "\n",
    "        # --- Training Loop ---\n",
    "        best_val_loss = float('inf') # Initialize with infinity for early stopping\n",
    "        epochs_no_improve = 0 # Counter for early stopping\n",
    "        best_model_state = None # To store the state of the best model\n",
    "\n",
    "        print(\"Starting Training...\")\n",
    "        for epoch in range(EPOCHS):\n",
    "            model.train() # Set model to training mode\n",
    "            train_loss = 0.0\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "            \n",
    "            for features, labels in train_loader:\n",
    "                features, labels = features.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "                # Zero gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Track loss and accuracy\n",
    "                train_loss += loss.item() * features.size(0)\n",
    "                predicted = (outputs > 0.5).float() # Convert probabilities to 0/1\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "            epoch_train_loss = train_loss / len(train_loader.dataset)\n",
    "            epoch_train_acc = correct_train / total_train\n",
    "\n",
    "            # --- Validation Step ---\n",
    "            model.eval() # Set model to evaluation mode\n",
    "            val_loss = 0.0\n",
    "            correct_val = 0\n",
    "            total_val = 0\n",
    "            with torch.no_grad(): # Disable gradient calculation for validation\n",
    "                for features, labels in val_loader:\n",
    "                    features, labels = features.to(DEVICE), labels.to(DEVICE)\n",
    "                    outputs = model(features)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_loss += loss.item() * features.size(0)\n",
    "                    predicted = (outputs > 0.5).float()\n",
    "                    total_val += labels.size(0)\n",
    "                    correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "            epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "            epoch_val_acc = correct_val / total_val\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f} | Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.4f}\")\n",
    "\n",
    "            # --- Learning Rate Scheduler Step ---\n",
    "            scheduler.step(epoch_val_loss)\n",
    "\n",
    "            # --- Early Stopping Check ---\n",
    "            if epoch_val_loss < best_val_loss:\n",
    "                best_val_loss = epoch_val_loss\n",
    "                epochs_no_improve = 0\n",
    "                # Save the best model state\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "                # print(f\"--- Best validation loss improved to {best_val_loss:.4f} ---\")\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve == EARLY_STOPPING_PATIENCE:\n",
    "                    print(f\"Early stopping triggered after {EARLY_STOPPING_PATIENCE} epochs with no improvement in validation loss.\")\n",
    "                    break # Stop training loop\n",
    "\n",
    "        print(\"Training Finished.\")\n",
    "\n",
    "        # Load the best model state before prediction\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "            print(\"Loaded best model state based on validation loss.\")\n",
    "        else:\n",
    "            print(\"No improvement in validation loss, using the last model state.\")\n",
    "\n",
    "\n",
    "        # --- Prediction on Final Test Set ---\n",
    "        model.eval() # Ensure model is in evaluation mode\n",
    "        all_predictions = []\n",
    "        with torch.no_grad():\n",
    "            for features in test_loader:\n",
    "                features = features.to(DEVICE)\n",
    "                outputs = model(features)\n",
    "                predicted_probs = outputs.cpu().numpy() # Move to CPU and convert to numpy\n",
    "                predicted_classes = (predicted_probs > 0.5).astype(int) # Threshold at 0.5\n",
    "                all_predictions.extend(predicted_classes.flatten()) # Flatten in case of single-column output\n",
    "\n",
    "        print(\"Making predictions with the Neural Network model...\")\n",
    "        \n",
    "        # --- Output Generation ---\n",
    "        output_df_nn = pd.DataFrame(all_predictions, columns=None)\n",
    "        output_filename_nn = 'answers.csv' # Changed filename to 'answers.csv' as required\n",
    "        try:\n",
    "            output_df_nn.to_csv(output_filename_nn, index=False, header=False)\n",
    "            print(f\"Neural Network predictions saved to {output_filename_nn}\")\n",
    "\n",
    "            # --- Feedback based on best validation loss (proxy for generalization) ---\n",
    "            # Note: We don't have the true test accuracy here, but validation loss is a good indicator\n",
    "            print(\"Validation metrics from the epoch with the best validation loss:\")\n",
    "            # You would need to track best validation accuracy alongside best validation loss\n",
    "            # For simplicity here, we just report the best loss and remind about the target\n",
    "            print(f\"Best Validation Loss achieved: {best_val_loss:.4f}\")\n",
    "            print(\"Goal is to achieve accuracy between 86.5% and 91.5% on the unseen test set.\")\n",
    "            print(f\"Your previous score suggests you are very close to the target range.\")\n",
    "            print(f\"Try submitting the '{output_filename_nn}' file generated by this code.\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving Neural Network predictions to {output_filename_nn}: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Error: Training and test feature columns do not match.\")\n",
    "else:\n",
    "    print(\"Error: 'target' column not found in train.csv or columns mismatch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8168aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "print(' ')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
